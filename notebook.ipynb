{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251e0353-9c1e-4a4e-9d94-0f013bcf9423",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tweet Classifier\n",
    "[tweets](https://www.kaggle.com/kazanova/sentiment140)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327a1eb-7e2d-4016-8250-0d35d2b9dc7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utilities, Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b96f35ef-31e4-4026-8aa3-cfde4a6066ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f04d5bd9-1358-4333-8ec3-3b9a9f757a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data = data[['sentiment', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7af409-1459-42fb-9b59-59800db7e399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a41666d0-63b7-47b9-8139-3dd404fbb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "#nltk.download(\"stopwords\")\n",
    "MAXLEN = 35\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "def change_sentiments(data):\n",
    "    \"\"\" The sentiment from the input data is 0 for unhappy, 4 for happy. Let's change the 4 to 1 \"\"\"\n",
    "    data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x==4 else 0)\n",
    "    return data\n",
    "\n",
    "def clean_tweet(tweet:str):\n",
    "    \"\"\" Removing punctuation, hashtags, lowercasing everything. The link remover needs to be fixed, as it currently deletes every word after the link \"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]* ', '', str(tweet)) #TODO: fix this link remover, it currently deletes everything beyond the link\n",
    "    tweet = re.sub(r'#', '', str(tweet)) #remove hashtab\n",
    "    \n",
    "    #remove punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    tweet = \"\".join(ch for ch in tweet if ch not in punct)\n",
    "    return tweet\n",
    "\n",
    "def make_numpy(in_data):\n",
    "    \"\"\"turns the relevant columns in pandas dataframe into numpy arrays\"\"\"\n",
    "    tweets = in_data['tweet'].to_numpy()\n",
    "    sentiments = in_data['sentiment'].to_numpy()\n",
    "    return tweets, sentiments\n",
    "\n",
    "def preprocess(tokenizer, tweets):\n",
    "    \"\"\"tokenizes and pads the tweets. note that tweets is a list of strings\"\"\"\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded = pad_sequences(sequences, truncating='post', padding='post', maxlen=MAXLEN)\n",
    "    return padded\n",
    "\n",
    "def make_tokenizer(vocab_size):\n",
    "    \"\"\" make the tokenizer \"\"\"\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(tweets)\n",
    "    return tokenizer\n",
    "\n",
    "def save_tokenizer(tokenizer):\n",
    "    \"\"\"save the tokenizer for future use\"\"\"\n",
    "    import pickle\n",
    "    with open('src/models/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_tokenizer():\n",
    "    with open('src/models/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model():   \n",
    "    \"\"\" Create a bidirectional LSTM model for sentiment analysis \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, 16, input_length=MAXLEN),\n",
    "        tf.keras.layers.Dropout(.2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True)),\n",
    "        layers.Dropout(0.2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40)),\n",
    "        layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def shorten_dataset(data, n=5000):\n",
    "    \"\"\" Shuffle and shorten the dataset in case you want to test \"\"\"\n",
    "    data = data.sample(frac=1)\n",
    "    data = data.head(n)\n",
    "    return data\n",
    "\n",
    "def batch_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Shuffle and batch the training and test data\"\"\"\n",
    "    BATCH_SIZE = 512\n",
    "    BUFFER_SIZE = 10000\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    dataset_train = dataset_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset_test = dataset_test.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3aed8171-8f65-4439-9a6b-1ca1d3b0a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/tweets.csv', encoding = 'Latin-1', names=('sentiment','id','date','flag','username','tweet'))\n",
    "#data = pd.read_csv('https://jtctweetdata.s3.us-east-1.amazonaws.com/tweets.csv?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMiJGMEQCIGpKZAv9QCUNci2DP37nydyAGzstLxqg9ibz8kzTyk5AAiA%2BObLbBl8QocsbFm3cV39KStJRb%2FIlCheXWpLyIuKuvCqeAgi%2B%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDc0MjA0ODQ2MzU2OCIMN5LUZT%2BhmWJb8RBCKvIBrD%2FROVZv1iq1V60URzynfAMVs1fQ4yk2vP%2FYFUzEsJv26m%2Fsk2HgtpdR8kHfDAfLKGcSXoiYH%2B7sM7c3wGfA%2BaoHLJujHSHYco2Coqizs8qB22vBFDbrysVoMZav29RP0BHg8D%2FvziQxPUqqBZoBl3y34vebIUvzNDBongYjeYwnVnSTaH8tz2oFlU4QOKv%2FfjTWsSB5yqFQybdD%2FDmPtzpDetZL66C3geu%2Ft%2BFFOJbWTkKT3f7vmMy61exbEsZwRycGrXrVdo7TLZFo1O4qHO%2FUzfywQX6aO2HNdOteMLxtbQiZ1FsX900cpCualWkAClowisOVjwY64AG2MJX8mbs4NvsJBsrYN45NijEqeMtsuHa29VyNNY5fv0T1dLBD%2Ff0bP4KhS1VbqZnJ0h%2BpMDRTorIbn8GJUtP0AdyG8Xvn6w4sFku90nNoO3T2bZhAcrtBJid8p%2BDMMwToTk6rKNfRm69mhKbuGEISx%2B6N7JeezOYRU0plnsx6vh%2Bdo7C4Zff1YQIZQizgCZdgQKnlzig6CjbuDrF6tNfPdUn9inslb1UX%2B6TxqsR2pijKwIcihITOttvJr02Xc3GTXSWS4XFBZAq4axyqYdntdqDghae1EZTCnxzVhyj2qg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220117T124319Z&X-Amz-SignedHeaders=host&X-Amz-Expires=432000&X-Amz-Credential=ASIA2ZRMI7LILP54HZFI%2F20220117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=4a7f3960910c6ce5184a5dbb7210b007511fe2045ad1abffae5db915baf382c7', encoding = 'Latin-1', names=('sentiment','id','date','flag','username','tweet'))\n",
    "data = shorten_dataset(data, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d72d9e08-e02e-4759-be63-527ebebce3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = change_sentiments(data)\n",
    "data['tweet'] = data['tweet'].apply(clean_tweet)\n",
    "tweets, sentiments = make_numpy(data)\n",
    "tokenizer = make_tokenizer(VOCAB_SIZE)\n",
    "padded_tweets = preprocess(tokenizer, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e68b3f73-d871-4aa4-8b40-b8ce30e28981",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce7f144e-dece-4c0e-893f-53ab2d181a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_tweets, sentiments, test_size=.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "dataset_train, dataset_test = batch_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7143c59b-26d7-42e5-a774-919ca819bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "92/92 [==============================] - 17s 149ms/step - loss: 0.6028 - accuracy: 0.6606 - val_loss: 0.4974 - val_accuracy: 0.7686\n",
      "Epoch 2/2\n",
      "92/92 [==============================] - 12s 135ms/step - loss: 0.4701 - accuracy: 0.7820 - val_loss: 0.4925 - val_accuracy: 0.7695\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(dataset_train, validation_data = dataset_test, epochs=2, \n",
    "              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2)]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a9b538b-5e11-4ac3-a906-dd48366a89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('src/models/tweet_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3f8030a-893c-4eba-a6ae-6305f8895e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model('src/models/tweet_classifier.h5')\n",
    "saved_tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2e3c405-06d2-4f05-b5fd-bcd7e2ad22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = [tweets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17a93c55-ee93-4374-aaf9-d4c4a51b217f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcf76641-2006-4195-9244-2155c86759a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28, 176,   9, 298, 995,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55c0b376-99a2-4eab-a855-a659cebb1d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7171427]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "910e92d9-8842-4d50-85f6-fe87a5f9b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this show is such self parody '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf42f50b-69c1-4740-ad14-f1883f1d9620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jazzminnie but ive done barely any revision so many things have bugged me that have completely put me off it '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055aefe2-02fa-4f44-8e32-198e5258d16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
